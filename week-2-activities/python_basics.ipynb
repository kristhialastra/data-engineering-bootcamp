{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ebed391",
   "metadata": {},
   "source": [
    "# 1. Write a Python function to normalize text from the Netflix dataset:\n",
    "- strip leading/trailing whitespace,\n",
    "- convert to lowercase,\n",
    "- replace common special characters from fields like title or description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32c28c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Netflix dataset...\n",
      "======================================================================\n",
      "\n",
      "Row 1 - title:\n",
      "  Before: Chocolate...\n",
      "  After:  chocolate...\n",
      "\n",
      "Row 1 - description:\n",
      "  Before: Brought together by meaningful meals in the past and present...\n",
      "  After:  brought together by meaningful meals in the past and present...\n",
      "\n",
      "Row 2 - title:\n",
      "  Before: Guatemala: Heart of the Mayan World...\n",
      "  After:  guatemala heart of the mayan world...\n",
      "\n",
      "Row 2 - description:\n",
      "  Before: From Sierra de las Minas to Esquipulas, explore Guatemala's ...\n",
      "  After:  from sierra de las minas to esquipulas, explore guatemalas c...\n",
      "\n",
      "Row 3 - title:\n",
      "  Before: The Zoya Factor...\n",
      "  After:  the zoya factor...\n",
      "\n",
      "Row 3 - description:\n",
      "  Before: A goofy copywriter unwittingly convinces the Indian cricket ...\n",
      "  After:  a goofy copywriter unwittingly convinces the indian cricket ...\n",
      "\n",
      "Row 4 - title:\n",
      "  Before: Atlantics...\n",
      "  After:  atlantics...\n",
      "\n",
      "Row 4 - description:\n",
      "  Before: Arranged to marry a rich man, young Ada is crushed when her ...\n",
      "  After:  arranged to marry a rich man, young ada is crushed when her ...\n",
      "\n",
      "Row 5 - title:\n",
      "  Before: Chip and Potato...\n",
      "  After:  chip and potato...\n",
      "\n",
      "Row 5 - description:\n",
      "  Before: Lovable pug Chip starts kindergarten, makes new friends and ...\n",
      "  After:  lovable pug chip starts kindergarten, makes new friends and ...\n",
      "\n",
      "Processed 1000 rows...\n",
      "\n",
      "Processed 2000 rows...\n",
      "\n",
      "Processed 3000 rows...\n",
      "\n",
      "Processed 4000 rows...\n",
      "\n",
      "Processed 5000 rows...\n",
      "\n",
      "======================================================================\n",
      "✓ Cleaning complete!\n",
      "✓ Total rows cleaned: 5837\n",
      "✓ Output saved to: netflix_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Clean/normalize a single text field\n",
    "    \"\"\"\n",
    "    if text is None or text == '':\n",
    "        return ''\n",
    "    \n",
    "    # Strip whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace special characters (keep letters, numbers, spaces, and commas for CSV)\n",
    "    text = re.sub(r'[^a-z0-9\\s,]', '', text)\n",
    "    \n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def clean_netflix_dataset(input_filepath, output_filepath):\n",
    "    \"\"\"\n",
    "    Read the Netflix CSV, clean text fields, and save to new file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fields na gusto i-normalize (title and description)\n",
    "    fields_to_clean = ['title', 'description']\n",
    "    \n",
    "    cleaned_count = 0\n",
    "    \n",
    "    try:\n",
    "        with open(input_filepath, 'r', encoding='utf-8') as infile:\n",
    "            csv_reader = csv.DictReader(infile)\n",
    "            \n",
    "            # Get all column names\n",
    "            fieldnames = csv_reader.fieldnames\n",
    "            \n",
    "            # Open output file for writing\n",
    "            with open(output_filepath, 'w', encoding='utf-8', newline='') as outfile:\n",
    "                csv_writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "                \n",
    "                # Write header\n",
    "                csv_writer.writeheader()\n",
    "                \n",
    "                print(\"Cleaning Netflix dataset...\")\n",
    "                print(\"=\"*70)\n",
    "                \n",
    "                # Process each row\n",
    "                for i, row in enumerate(csv_reader):\n",
    "                    \n",
    "                    # Clean the specified fields\n",
    "                    for field in fields_to_clean:\n",
    "                        if field in row and row[field]:\n",
    "                            original = row[field]\n",
    "                            row[field] = normalize_text(original)\n",
    "                            \n",
    "                            # Show first 5 examples lang para makita mo yung changes\n",
    "                            if i < 5:\n",
    "                                print(f\"\\nRow {i+1} - {field}:\")\n",
    "                                print(f\"  Before: {original[:60]}...\")\n",
    "                                print(f\"  After:  {row[field][:60]}...\")\n",
    "                    \n",
    "                    # Write cleaned row to output file\n",
    "                    csv_writer.writerow(row)\n",
    "                    cleaned_count += 1\n",
    "                    \n",
    "                    # Progress indicator every 1000 rows\n",
    "                    if (i + 1) % 1000 == 0:\n",
    "                        print(f\"\\nProcessed {i + 1} rows...\")\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*70)\n",
    "                print(f\"✓ Cleaning complete!\")\n",
    "                print(f\"✓ Total rows cleaned: {cleaned_count}\")\n",
    "                print(f\"✓ Output saved to: {output_filepath}\")\n",
    "                \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{input_filepath}' not found!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "# Run the cleaning\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r'netflix.csv'\n",
    "    output_file = r'netflix_cleaned.csv'\n",
    "    \n",
    "    clean_netflix_dataset(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845409a3",
   "metadata": {},
   "source": [
    "# 2. Parse the netflix_titles.csv file manually:\n",
    "- read the first 10 lines as raw text using Python's file handling,\n",
    "- extract specific fields (like title and release_year) from each line,\n",
    "- summarize basic contents (count lines, identify unique categories from a few lines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c7d75d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MANUAL CSV PARSING - ALL REQUIREMENTS\n",
      "\n",
      "======================================================================\n",
      "REQUIREMENT 1: Reading first 10 lines as RAW TEXT\n",
      "======================================================================\n",
      "\n",
      "Header (raw text):\n",
      "show_id,title,director,cast,country,date_added,release_year,rating,duration,listed_in,description,type\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Reading next 10 lines as RAW TEXT:\n",
      "\n",
      "Line 1 (raw): 81193313,Chocolate,,\"Ha Ji-won, Yoon Kye-sang, Jang Seung-jo, Kang Bu-ja, Lee Jae-ryong, Min Jin-woo...\n",
      "\n",
      "Line 2 (raw): 81197050,Guatemala: Heart of the Mayan World,\"Luis Ara, Ignacio Jaunsolo\",Christian Morales,,\"Novemb...\n",
      "\n",
      "Line 3 (raw): 81213894,The Zoya Factor,Abhishek Sharma,\"Sonam Kapoor, Dulquer Salmaan, Sanjay Kapoor, Sikander Khe...\n",
      "\n",
      "Line 4 (raw): 81082007,Atlantics,Mati Diop,\"Mama Sane, Amadou Mbow, Ibrahima Traore, Nicole Sougou, Amina Kane, Ma...\n",
      "\n",
      "Line 5 (raw): 80213643,Chip and Potato,,\"Abigail Oliver, Andrea Libman, Briana Buckmaster, Brian Dobson, Chance Hu...\n",
      "\n",
      "Line 6 (raw): 81172754,Crazy people,Moses Inwang,\"Ramsey Nouah, Chigul, Sola Sobowale, Ireti Doyle, Ben Touitou, F...\n",
      "\n",
      "Line 7 (raw): 81120982,I Lost My Body,Jérémy Clapin,\"Hakim Faris, Victoire Du Bois, Patrick d'Assumçao, Dev Patel,...\n",
      "\n",
      "Line 8 (raw): 81227195,Kalushi: The Story of Solomon Mahlangu,Mandla Dube,\"Thabo Rametsi, Thabo Malema, Welile Nzu...\n",
      "\n",
      "Line 9 (raw): 70205672,La Reina del Sur,,\"Kate del Castillo, Cristina Urgel, Alberto Jiménez, Juan José Arjona, Hu...\n",
      "\n",
      "Line 10 (raw): 81172841,Lagos Real Fake Life,Mike Ezuruonye,\"Nonso Diobi, Mike Ezuruonye, Mercy Aigbe, Rex Nosa, An...\n",
      "\n",
      "======================================================================\n",
      "REQUIREMENT 2: Extracting SPECIFIC FIELDS (title, release_year)\n",
      "======================================================================\n",
      "\n",
      "Line 1:\n",
      "  Title extracted: Chocolate...\n",
      "  Year extracted:   Kang Bu-ja\n",
      "\n",
      "Line 2:\n",
      "  Title extracted: Guatemala: Heart of the Mayan World...\n",
      "  Year extracted:  \"November 30\n",
      "\n",
      "Line 3:\n",
      "  Title extracted: The Zoya Factor...\n",
      "  Year extracted:   Sikander Kher\n",
      "\n",
      "Line 4:\n",
      "  Title extracted: Atlantics...\n",
      "  Year extracted:   Nicole Sougou\n",
      "\n",
      "Line 5:\n",
      "  Title extracted: Chip and Potato...\n",
      "  Year extracted:   Brian Dobson\n",
      "\n",
      "Line 6:\n",
      "  Title extracted: Crazy people...\n",
      "  Year extracted:   Ireti Doyle\n",
      "\n",
      "Line 7:\n",
      "  Title extracted: I Lost My Body...\n",
      "  Year extracted:   Dev Patel\n",
      "\n",
      "Line 8:\n",
      "  Title extracted: Kalushi: The Story of Solomon Mahlangu...\n",
      "  Year extracted:   Jafta Mamabolo\n",
      "\n",
      "Line 9:\n",
      "  Title extracted: La Reina del Sur...\n",
      "  Year extracted:   Juan José Arjona\n",
      "\n",
      "Line 10:\n",
      "  Title extracted: Lagos Real Fake Life...\n",
      "  Year extracted:   Rex Nosa\n",
      "\n",
      "======================================================================\n",
      "REQUIREMENT 3: SUMMARIZE BASIC CONTENTS\n",
      "======================================================================\n",
      "\n",
      "Total lines read: 11 (including header)\n",
      "Data lines processed: 10\n",
      "\n",
      "Unique content types found: {' Diankou Sembene\"', ' Odunlade Adekola', 'India', ' Salvador Zerboni', ' Scotia Anderson', ' 2019\"', 'South Korea', ' Patrick Onyeke\"', '\"Documentaries'}\n",
      "\n",
      "Type distribution from first 10 lines:\n",
      "  South Korea: 1\n",
      "  \"Documentaries: 1\n",
      "  India: 1\n",
      "   Diankou Sembene\": 1\n",
      "   Scotia Anderson: 1\n",
      "   Patrick Onyeke\": 1\n",
      "   2019\": 2\n",
      "   Salvador Zerboni: 1\n",
      "   Odunlade Adekola: 1\n",
      "\n",
      "No valid release years found in extracted data\n",
      "\n",
      "======================================================================\n",
      "✓ All three requirements completed!\n",
      "======================================================================\n",
      "\n",
      "✓ Parse completed successfully!\n",
      "✓ Captured 11 raw lines\n",
      "✓ Extracted 10 titles\n",
      "✓ Summarized 10 content types\n"
     ]
    }
   ],
   "source": [
    "def parse_netflix_csv_manual(filepath, num_lines=10):\n",
    "    \"\"\"\n",
    "    Manually parse CSV file without using pandas/csv library\n",
    "    Does ALL three requirements:\n",
    "    1. Read first 10 lines as raw text\n",
    "    2. Extract specific fields (title and release_year)\n",
    "    3. Summarize basic contents\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'raw_lines': [],           # para sa requirement 1: raw text\n",
    "        'titles': [],              # para sa requirement 2: extracted fields\n",
    "        'release_years': [],       # para sa requirement 2: extracted fields\n",
    "        'types': [],               # para sa requirement 3: categories\n",
    "        'total_lines': 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Open file in read mode\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "            print(\"=\"*70)\n",
    "            print(\"REQUIREMENT 1: Reading first 10 lines as RAW TEXT\")\n",
    "            print(\"=\"*70)\n",
    "            print()\n",
    "            \n",
    "            # Read header first\n",
    "            header_line = file.readline().strip()\n",
    "            results['raw_lines'].append(header_line)\n",
    "            results['total_lines'] += 1\n",
    "            \n",
    "            print(f\"Header (raw text):\")\n",
    "            print(header_line)\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print()\n",
    "            \n",
    "            # Parse header to find column positions\n",
    "            columns = header_line.split(',')\n",
    "            \n",
    "            # Read next num_lines lines AS RAW TEXT\n",
    "            print(f\"Reading next {num_lines} lines as RAW TEXT:\")\n",
    "            print()\n",
    "            \n",
    "            for i in range(num_lines):\n",
    "                line = file.readline()\n",
    "                \n",
    "                if not line:  # end of file\n",
    "                    break\n",
    "                \n",
    "                line = line.strip()\n",
    "                results['raw_lines'].append(line)\n",
    "                results['total_lines'] += 1\n",
    "                \n",
    "                # Show raw text (first 100 chars lang para hindi masyadong mahaba)\n",
    "                print(f\"Line {i+1} (raw): {line[:100]}...\")\n",
    "                print()\n",
    "        \n",
    "        # Now process those raw lines\n",
    "        print(\"=\"*70)\n",
    "        print(\"REQUIREMENT 2: Extracting SPECIFIC FIELDS (title, release_year)\")\n",
    "        print(\"=\"*70)\n",
    "        print()\n",
    "        \n",
    "        # Skip header (index 0), process data lines\n",
    "        for i, raw_line in enumerate(results['raw_lines'][1:], start=1):\n",
    "            # Simple split by comma (may issues with quoted commas, pero manual parsing nga eh)\n",
    "            fields = raw_line.split(',')\n",
    "            \n",
    "            # Try to extract fields based on position\n",
    "            # Column positions from header: show_id(0), title(1), ..., release_year(6), ..., type(11)\n",
    "            try:\n",
    "                title = fields[1] if len(fields) > 1 else 'N/A'\n",
    "                year = fields[6] if len(fields) > 6 else 'N/A'\n",
    "                content_type = fields[11] if len(fields) > 11 else 'N/A'\n",
    "                \n",
    "                # Store extracted fields\n",
    "                results['titles'].append(title)\n",
    "                results['release_years'].append(year)\n",
    "                results['types'].append(content_type)\n",
    "                \n",
    "                # Print extracted fields\n",
    "                print(f\"Line {i}:\")\n",
    "                print(f\"  Title extracted: {title[:50]}...\")\n",
    "                print(f\"  Year extracted:  {year}\")\n",
    "                print()\n",
    "                \n",
    "            except IndexError:\n",
    "                print(f\"Line {i}: Could not parse (not enough fields)\")\n",
    "                print()\n",
    "        \n",
    "        # Summarize basic contents\n",
    "        print(\"=\"*70)\n",
    "        print(\"REQUIREMENT 3: SUMMARIZE BASIC CONTENTS\")\n",
    "        print(\"=\"*70)\n",
    "        print()\n",
    "        \n",
    "        # Count lines\n",
    "        print(f\"Total lines read: {results['total_lines']} (including header)\")\n",
    "        print(f\"Data lines processed: {len(results['titles'])}\")\n",
    "        print()\n",
    "        \n",
    "        # Identify unique categories from a few lines\n",
    "        unique_types = set(results['types'])\n",
    "        print(f\"Unique content types found: {unique_types}\")\n",
    "        print()\n",
    "        \n",
    "        # Count each type\n",
    "        type_counts = {}\n",
    "        for t in results['types']:\n",
    "            type_counts[t] = type_counts.get(t, 0) + 1\n",
    "        \n",
    "        print(\"Type distribution from first 10 lines:\")\n",
    "        for content_type, count in type_counts.items():\n",
    "            print(f\"  {content_type}: {count}\")\n",
    "        print()\n",
    "        \n",
    "        # Show some statistics about the years\n",
    "        valid_years = [y for y in results['release_years'] if y.isdigit()]\n",
    "        if valid_years:\n",
    "            print(f\"Release years found: {', '.join(valid_years)}\")\n",
    "            print(f\"Year range: {min(valid_years)} to {max(valid_years)}\")\n",
    "        else:\n",
    "            print(\"No valid release years found in extracted data\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"✓ All three requirements completed!\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filepath}' not found!\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Run the parser\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = r'netflix.csv'\n",
    "    \n",
    "    print(\"MANUAL CSV PARSING - ALL REQUIREMENTS\")\n",
    "    print()\n",
    "    \n",
    "    results = parse_netflix_csv_manual(filepath, num_lines=10)\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\n✓ Parse completed successfully!\")\n",
    "        print(f\"✓ Captured {len(results['raw_lines'])} raw lines\")\n",
    "        print(f\"✓ Extracted {len(results['titles'])} titles\")\n",
    "        print(f\"✓ Summarized {len(results['types'])} content types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7411516d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
